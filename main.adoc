:!chapter-signifier:
:toc:
:toclevels: 6
:sectanchorts:
:sectnums:
:icons: font
:source-highlighter: rouge
:asciimath:
:doctype: book

= Αλγόριθμοι Σύστασης με Χρήση Τεχνητών Νευρωνικών Δικτύων και Τεχνικών Ομαδοποίησης Δεδομένων

== Θέμα 1: Προ-επεξεργασία Δεδομένων

Προτού προχωρήσουμε στις απαντήσεις των ερωτημάτων, φορτώνουμε τα δεδομένα ως ένα latexmath:[dataframe] χρησιμοποιώντας τις βιβλιοθήκες numpy και pandas της Python.

----
import numpy as np
import pandas as pd


def load_dataframe():

    # Load the dataset
    dataset = np.load('Dataset.npy')
    # Convert to dataframe
    X = pd.DataFrame(dataset)

    return X
----

Εκτυπώνοντας το latexmath:[head] του latexmath:[dataframe], έχουμε μια προεσκόπιση της δομής των δεδομένων μας (figure 1).

----
# Print the first five entries
X = load_dataframe()
print(X.head())
----

.Προεσκόπιση δεδομένων από Dataset.npy
image::img1.png[200, 400]

<<<


=== Ερώτημα 1

Βρίσκουμε τις στήλες latexmath:[users] (στήλη 1) και latexmath:[items] (στήλη 2) του latexmath:[dataframe] και καλούμε τη μέθοδο latexmath:[nunique] για να βρούμε τα πλήθη των μοναδικών χρηστών και αντικειμένων.

----

from dataset import load_dataframe

X = load_dataframe()

# Split the users' and items' columns
X['users'] = X[0].str.split(',').str[0]
X['items'] = X[0].str.split(',').str[1]

# Getting the unique count of users and items
unique_users = X['users'].nunique()
unique_items = X['items'].nunique()

print("Unique count of users:", unique_users)
print("Unique count of items:", unique_items)

----

Εκτυπώνοντας στην οθόνη βλέπουμε το παρακάτω αποτέλεσμα.

.Count of unique users and items
image::img2.png[200, 400]

<<<


=== Ερώτημα 2

Καλούμε τη μέθοδο latexmath:[load_dataframe] και υπολογίζουμε πόσες φορές εμφανίζεται κάθε χρήστης για να βρούμε τα άκρα των δεδομένων μας. Παρατηρούμε ότι κανένας χρήστης δεν έχει δώσει πάνω από 29 αξιολογήσεις (πάνω όριο).

Επιλέγουμε το υποσύνολο που βρίσκεται μεταξύ latexmath:[Rmin = 3] και latexmath:[Rmax = 50]. Τώρα τα δεδομένα μας έχουν μήκος 358.

Στη συνέχεια φτιάχνουμε το latexmath:[dataframe R] που προκύπτει από τους χρήστες που έχουμε φιλτράρει και δίνουμε στις στήλες του τα αντίστοιχα ονόματα: latexmath:[Users, Items, Ratings, Timestamp].

Τέλος, μετατρέπουμε τα δεδομένα των πρώτων τριών στηλών σε latexmath:[integers] και τα δεδομένα της τελευταίας στήλης σε latexmath:[datetime objects].

----

def R_dataframe():
    # Load the DataFrame
    X = load_dataframe()
    # Count the occurrences of each user
    user_counts = X.iloc[:, 0].value_counts()

    # Filter users who appear between Rmin = 3 and Rmax = 50 times (length: 358, for Rmax = 2 we'd have 8662)
    # no one has given more than 29 ratings
    filtered_users = user_counts[(user_counts >= 3) & (user_counts <= 50)]

    # Create R dataframe with rows of users appearing in filtered_users
    R = X[X.iloc[:, 0].isin(filtered_users.index)]
    # Splitting the single column into separate columns
    R = R[0].str.split(',', expand=True)
    R.columns = ['Users', 'Items', 'Ratings', 'Timestamp']

    # Convert to numeric values
    R['Ratings'] = pd.to_numeric(R['Ratings'])


    R['Users'] = R['Users'].str.extract('(\d+)').astype(int)
    R['Items'] = R['Items'].str.extract('(\d+)').astype(int)

    # Convert 'Timestamp' column to datetime object
    R['Timestamp'] = pd.to_datetime(R['Timestamp'])
    return R


# Print R
R = R_dataframe()
print(R)

----

<<<

Εκτυπώνοντας στην οθόνη βλέπουμε το παρακάτω αποτέλεσμα.

.Προεσκόπιση δεδομένων του R dataframe
image::img3.png[200, 400]

<<<

=== Ερώτημα 3

Για να αναπαραστήσουμε γραφικά τα ιστογράμματα του ερωτήματος, έχουμε χρησιμοποιήσει τη βιβλιοθήκη latexmath:[matplotlib] της Python.

==== Ιστόγραμμα συχνότητας για το πλήθος των αξιολογήσεων του κάθε χρήστη.

Φορτώνουμε το latexmath:[dataframe R] όπως υπολογίστηκε στο υπο-ερώτημα 2 και ομαδοποιούμε τα δεδομένα του ως προς τις αξιολογήσεις ανά μοναδικό χρήστη και φτιάχνουμε το ιστόγραμμα.

....

# Load the updated dataframe that occurred from Q2 where Rmin = 3 and Rmax = 50
R = R_dataframe()

# Grouping by the values of ratings and users
ratings_per_user = R.groupby('Ratings')['Users'].nunique()

plt.hist(ratings_per_user, bins=50)

# Plotting the histogram
ratings_per_user.plot(kind='bar', stacked=True)

# Setting labels and title
plt.xlabel('Ratings')
plt.ylabel('No. of Users')
plt.title('Histogram')

# Display the plot
plt.show()

....


Εκτυπώνοντας στην οθόνη βλέπουμε το παρακάτω αποτέλεσμα.

.Ιστόγραμμα συχνότητας για το πλήθος των αξιολογήσεων του κάθε χρήστη
image::img4.png[]

<<<

==== Ιστόγραμμα συχνότητας για το χρονικό εύρος των αξιολογήσεων του κάθε χρήστη.

Φορτώνουμε το latexmath:[dataframe R] όπως υπολογίστηκε στο υπο-ερώτημα 2 και ομαδοποιούμε τα δεδομένα των χρηστών υπολογίζοντας το ελάχιστο και μέγιστο διάστημα σε μέρες που μεσολάβησε για να δοθούν όλες οι αξιολογήσεις ανά χρήστη.

Τέλος, προσαρμόζουμε τις παραμέτρους της γραφικής αναπαράστασης του ιστογράμματος ώστε να είναι πιο ευανάγνωστη η πληροφορία.

....

# Load the updated dataframe that occurred from Q2 where Rmin = 3 and Rmax = 50
R = R_dataframe()

# Plotting the histogram
user_timestamp_range = (R.groupby('Users')['Timestamp']
                        .agg(lambda x: (x.max() - x.min()).days))

values, bins, _ = plt.hist(user_timestamp_range, bins=50)

# Showing only time ranges that matches user activity (non-zero values).
nonzero_indices = [i for i, val in enumerate(values) if val != 0]
plt.xticks(bins[nonzero_indices])

# Showing count of users for each time range
for i in range(len(bins) - 1):
    if values[i] != 0:  # Check if the count is non-zero
        plt.text(bins[i] + (bins[i+1] - bins[i]) / 2, values[i], str(int(values[i])), ha='center', va='bottom')

# Setting labels and title
plt.xlabel('Time range in days')
plt.ylabel('No. of Users')
plt.title('Histogram')

# Display the plot
plt.show()

....


Εκτυπώνοντας στην οθόνη βλέπουμε το παρακάτω αποτέλεσμα.

.Ιστόγραμμα συχνότητας για το χρονικό εύρος των αξιολογήσεων του κάθε χρήστη.
image::img5.png[]

<<<

== Θέμα 2: Αλγόριθμοι Ομαδοποίησης Δεδομένων

=== Ερώτημα 1

Φορτώνουμε τις σχετικές βιβλιοθήκες και τα δεδομένα μας.
....

from pre_process_2 import R_dataframe
from sklearn.cluster import KMeans
import numpy as np
from sklearn.metrics import pairwise_distances
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

R = R_dataframe()

....

Δημιουργούμε έναν πίνακα όπου οι σειρές αντιπροσωπεύουν χρήστες και οι στήλες αντικείμενα. Οι τιμές του πίνακα είναι οι βαθμολογίες που έδωσαν οι χρήστες στα αντικείμενα, με 0 όπου δεν υπάρχει βαθμολογία.
....

# Dataframe where each row represents a user and each column represents an item
R = R.pivot_table(index='Users', columns='Items', values='Ratings', fill_value=0)

....
Δημιουργούμε τα feature vectors.
....
# Feature vectors for each user
feature_vectors = R.values
....

Ορίζουμε τον αριθμό των clusters και δημιουργούμε τα binary vectors τα οποία μετατρέπουν τα χαρακτηριστικά σε δυαδική μορφή (1 αν υπάρχει βαθμολογία, 0 αν δεν υπάρχει), δημιουργώντας δυαδικά διανύσματα για κάθε χρήστη.

....
# Set the number of clusters (L)
L = 3
def compute_binary_vector(feature_vectors):
    binary_vectors = np.where(feature_vectors > 0, 1, 0)
    return binary_vectors

binary_vectors = compute_binary_vector(feature_vectors)

....

Υπολογίζουμε την προσαρμοσμένη ευκλείδεια απόσταση μεταξύ των χρηστών, λαμβάνοντας υπόψη τα δυαδικά διανύσματα (δηλαδή ποια αντικείμενα έχουν βαθμολογηθεί).

....
# euclidean distance based on equation
def euclidean_distance(R_u, R_v, lambda_u, lambda_v):
    # Υπολογισμός της απόστασης με βάση τη συνθήκη λj(k)
    squared_diff = np.sum(((R_u - R_v) ** 2) * lambda_u * lambda_v)
    distance = np.sqrt(squared_diff)
    return distance


# Compute pairwise distances using the custom distance function
custom_distances = pairwise_distances(feature_vectors, metric=euclidean_distance, **{'lambda_u': binary_vectors, 'lambda_v': binary_vectors})

print(custom_distances)

# Check the shapes of feature_vectors and custom_distances
print("Feature vectors shape:", feature_vectors.shape)
print("Binary vectors shape:", binary_vectors.shape)
print("Custom distances shape:", custom_distances.shape)

....

Μετατρέπουμε τον πίνακα αποστάσεων σε ένα μονοδιάστατο πίνακα και τυπώνουμε τα αποτελέσματα.
....

#Flatten custom distances array to create a 1D array of weights
custom_distances_1d = custom_distances.flatten()
print("Custom distances 1d shape:", custom_distances_1d.shape)

#print("Reduced custom distances shape:", reduced_custom_distances.shape)


print("Feature vectors:", feature_vectors)
print("Binary vectors:", binary_vectors)
print("Custom distances:", custom_distances)

print("Custom distances 1d:", custom_distances_1d)

....

Με τον αλγόριθμο KMeans, ομαδοποιούμε τους χρήστες σε L clusters με βάση τα διανύσματα χαρακτηριστικών.

....
# Initialize Kmeans
kmeans = KMeans(n_clusters=L, init='k-means++')

# Fit kmeans to data with the custom distance
kmeans_result = kmeans.fit(feature_vectors)
#print kmeans_result

....

Με την τεχνική Principal Component Analysis (PCA) μειώνουμε τις διαστάσεις των δεδομένων σε δύο, για να είναι δυνατή η οπτικοποίηση των clusters σε ένα 2D διάγραμμα.
....
pca = PCA(n_components=2)
pca_result = pca.fit_transform(feature_vectors)
#print(pca_result)

cluster_labels = kmeans.labels_

....

Δημιουργούμε ένα διάγραμμα διασποράς όπου κάθε σημείο αντιπροσωπεύει έναν χρήστη και το χρώμα του αντιστοιχεί στο cluster στο οποίο ανήκει.

....
# Visualize clusters for n_clusters = 3
plt.figure(figsize=(10, 6))
plt.scatter(pca_result[:, 0], pca_result[:, 1], c=cluster_labels, cmap='viridis', alpha=0.5)
plt.title('User Clusters (n_clusters = 3)')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar()
plt.show()

....

==== Διάγραμμα Διασποράς Ομαδοποίησης

Τρέχοντας τον κώδικα θα έχουμε το παρακάτω αποτέλεσμα ομαδοποίησης των χρηστών σε clusters.

.Clustering Scatterplot
image::2.1.png[]

<<<

== Θέμα 3: Αλγόριθμοι Παραγωγής Συστάσεων με Χρήση Τεχνητών Νευρωνικών Δικτύων

Εισάγουμε τις βιβλιοθήκες latexmath:[sklearn.cluster] και latexmath:[sklearn.metrics] και φορτώνουμε τα δεδομένα μας από τη συνάρτηση latexmath:[R_dataframe()].
....

R = R_dataframe()

....

Τα δεδομένα μετασχηματίζονται σε έναν πίνακα όπου κάθε σειρά αντιπροσωπεύει έναν χρήστη (user) και κάθε στήλη ένα αντικείμενο (item). Οι τιμές είναι οι βαθμολογίες (ratings).
....

# Dataframe where each row represents a user and each column represents an item
R = R.pivot_table(index='Users', columns='Items', values='Ratings', fill_value=0)

....

Εξάγουμε από το dataframe τα διανύσματα χαρακτηριστικών (feature vectors) για κάθε χρήστη.
....

# Feature vectors for each user
feature_vectors = R.values

....

Ορίζουμε τη συνάρτηση latexmath:[custom_jaccard_coefficient(u, v)] για να υπολογίσουμε το συντελεστή Jaccard μεταξύ δύο διανυσμάτων.

Στον ονομαστή του συντελεστή έχουμε τις ταινίες που αξιολόγησαν από κοινού οι χρήστες u και v και στον παρονομαστή την ένωση των ταινιών που αξιολόγησαν συνολικά.
....

# Custom Jaccard coefficient computation function
def custom_jaccard_coefficient(u, v):
    intersection = np.intersect1d(u, v)
    union = np.union1d(u, v)
    coefficient = 1.0 - len(intersection) / len(union)
    return coefficient

....

Ορίζουμε τον αριθμό των ομάδων (clusters) και αρχικοποιούμε τον αλγόριθμο latexmath:[KMeans].

....

# Set the number of clusters (L)
L = 3

# Initialize KMeans
kmeans = KMeans(n_clusters=L, init='k-means++')

....

Υπολογίζουμε τις αποστάσεις latexmath:[Jaccard].

....

# Compute the pairwise Jaccard distances between all pairs of users
pairwise_jaccard_distances = pairwise_distances(feature_vectors, metric=custom_jaccard_coefficient)

....

Εκπαιδεύουμε τον latexmath:[KMeans].

....

# Fit KMeans to the pairwise Jaccard distances
cluster_labels = kmeans.fit_predict(pairwise_jaccard_distances)
....

Εκτυπώνουμε τις ομάδες (clusters).

....

# Print the clusters
for cluster_idx in range(L):
    print("Cluster", cluster_idx, ":")
    cluster_users = np.where(cluster_labels == cluster_idx)[0]
    for user_idx in cluster_users:
        print("User", user_idx)
    print()

....

Ο κώδικας κατατάσσει τους χρήστες σε 3 ομάδες (clusters) χρησιμοποιώντας τον αλγόριθμο KMeans και αποστάσεις Jaccard, και εκτυπώνει τους χρήστες που ανήκουν σε κάθε ομάδα όπως φαίνονται παρακάτω.

.Cluster 1
image::3.1.1.png[]

.Cluster 2
image::3.1.2.png[]

.Cluster 3
image::3.1.3.png[]

<<<


=== Ερώτημα 1

Η μετρική αυτή ποσοτικοποιεί το πλήθος των κοινών αξιολογήσεων δυο χρηστών. Το κλάσμα μπορεί να πάρει τιμές απο 0 εως 1. Αν δύο χρήστες δεν έχουν καμία κοινή αξιολόγηση η απόσταση θα είναι ίση με 1. Διαφορετικά, αν έχουν αξιολογήσει και οι δυο το ίδιο σύνολο ταινιών η απόσταση θα είναι ιση με 0.

Κατά συνέπεια, η συγκεκριμένη μετρική δεν εστιάζει στο αν ο χρήστης u με το χρήστη v είχαν παρόμοιο τρόπο αξιολόγησης (αν τα γούστα τους δηλαδή μοιάζουν ή αν συσχετίζονται). Αγνοεί δηλαδή τις διαφορές στις τιμές των βαθμολογιών. Στην πραγματικότητα, αποδίδει μικρή απόσταση ανάμεσα σε δυο χρηστές όταν αυτοί έχουν αξιολογήσει το ίδιο σύνολο ταινιών και μεγαλύτερη απόσταση όσο τα σύνολα διαφοροποιούνται. Συνεπώς, με τη μετρική αυτή έχουμε απώλεια πληροφοριών που αφορούν στην ποσότητα.

Σε αντίθεση με τη μετρική Jaccard, η ευκλείδεια απόσταση λαμβάνει υπόψη τις διαφορές στις τιμές των βαθμολογιών. Είναι μια κλασική μετρική απόστασης που χρησιμοποιείται ευρέως και είναι καλά κατανοητή.

Η απόσταση συνημιτόνου (cosine similarity) μας δίνει το προφίλ ενός χρήστη συναρτήσει ενός άλλου με βάση την κατεύθυνση των διανυσμάτων. Συγκεκριμένα, ανάλογα με το πρόσημο του παράγοντα λ έχουμε θετική ή αρνητική συσχέτιση. Η μετρική αυτή, αν και πιο πολύπλοκη από την ευκλείδεια απόσταση, μπορεί να είναι χρήσιμη σε δεδομένα που η ομοιότητα κατεύθυνσης είναι πιο σημαντική από την ομοιότητα μεγέθους.

Τελικά, η επιλογή της κατάλληλης μετρικής εξαρτάται από τα χαρακτηριστικά των δεδομένων και το στόχο της ομαδοποίησης.

<<<


=== Ερώτημα 2

Εισάγουμε από τη βιβλιοθήκη latexmath:[tensorflow.keras.models] το μοντέλο *_Sequential_*, από τη βιβλιοθήκη latexmath:[tensorflow.keras.layers] το layer *_Dense_* και φορτώνουμε τα δεδομένα μας από τη συνάρτηση latexmath:[R_dataframe()].

....
# Load and preprocess data
R = R_dataframe()
R = R.pivot_table(index='Users', columns='Items', values='Ratings', fill_value=0)
feature_vectors = R.values
....

Χρησιμοποιούμε τη μετρική Jaccard από το προηγούμενο ερώτημα για να υπολογίσουμε τις αποστάσεις μεταξύ των χρηστών και στη συνέχεια εφαρμόζουμε τον αλγόριθμο K-Means για να δημιουργήσουμε 3 clusters.

....
# Number of clusters
L = 3
# Number of nearest neighbors
k = 5

# Initialize KMeans
kmeans = KMeans(n_clusters=L, init='k-means++')
pairwise_jaccard_distances = pairwise_distances(feature_vectors, metric=custom_jaccard_coefficient)
cluster_labels = kmeans.fit_predict(pairwise_jaccard_distances)
....

Για κάθε χρήστη, βρίσκουμε τους latexmath:[k] πλησιέστερους γείτονές του εντός του ίδιου cluster. Για κάθε cluster, εκπαιδεύουμε ένα MLP (Multi Layer Perceptron) με είσοδο τις προτιμήσεις των latexmath:[k] πλησιέστερων γειτόνων και έξοδο τις προτιμήσεις του χρήστη.

....
# Train an MLP for each cluster
mlp_models = []

for cluster_idx in range(L):
    print(f"Training MLP for cluster {cluster_idx}...")

    # Get users in the current cluster
    cluster_users = np.where(cluster_labels == cluster_idx)[0]
    cluster_data = feature_vectors[cluster_users]

    # Find k nearest neighbors for each user in the cluster
    knn = NearestNeighbors(n_neighbors=k, metric=custom_jaccard_coefficient)
    knn.fit(cluster_data)
    neighbors = knn.kneighbors(cluster_data, return_distance=False)

    # Prepare training data
    X_train = []
    y_train = []

    for user_idx, user_neighbors in enumerate(neighbors):
        X_train.append(cluster_data[user_neighbors])
        y_train.append(cluster_data[user_idx])

    X_train = np.array(X_train).reshape(len(cluster_users), -1)
    y_train = np.array(y_train)

    # Build and train MLP
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1],)))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(y_train.shape[1], activation='linear'))

    model.compile(optimizer='adam', loss='mean_squared_error')
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

    mlp_models.append(model)

....

Αποθηκεύουμε τα εκπαιδευμένα μοντέλα σε αρχεία.

....
# Save models
for idx, model in enumerate(mlp_models):
    model.save(f'mlp_cluster_{idx}.keras')

print("Training complete!")

....

Προβάλλουμε μια περίληψη του μοντέλου για ένα από τα clusters.

....
# Load and display one of the saved models
model = load_model('mlp_cluster_0.keras')
model.summary()
....

==== Ερμηνεία Αποτελεσμάτων

Τα παρακάτω αποτελέσματα περιλαμβάνουν πληροφορίες σχετικά με τη διαδικασία της εκπαίδευσης.

Βλέπουμε ότι η εκπαίδευση τρέχει για 10 εποχές και ότι η απώλεια σταδιακά μειώνεται, άρα το μοντέλο βελτιώνει σταδιακά τις προβλέψεις του.

.Training Models for Clusters
image::3.2.1.png[]


Στην περίληψη που βλέπουμε παρακάτω έχουμε μια αναφορά στην αρχιτεκτονική του μοντέλου. Συγκεκριμένα, βλέπουμε λεπτομέρειες για τα στρώματα του μοντέλου (2 στρώματα).

Το σχήμα της εξόδου για το πρώτο στρώμα έχει διάσταση 32 νευρώνες και αποτελείται από 36,512 παραμέτρους, ενώ για το δεύτερο στρώμα έχουμε 228 νευρώνες και 7,524 παραμέτρους. Ο αριθμός των παραμέτρων υπολογίζεται ως latexmath:[είσοδοι x νευρωνες + νευρωνες].

Τέλος, βλέπουμε ότι ο συνολικός αριθμός των παραμέτρων στο μοντέλο είναι 132,110 και ο αριθμός των παραμέτρων που εκπαιδεύονται είναι 44,036.

.Model Summary for Cluster 0
image::3.2.2.png[]

<<<

=== Ερώτημα 3 & 4

Εδώ έχουμε τον ίδιο κώδικα με το προηγούμενο ερώτημα με παραλλαγή της κεντρικής συνάρτησης.

Συγκεκριμένα, δημιουργούμε τους πίνακες X και Υ, όπου το X περιέχει τα διανύσματα χαρακτηριστικών των γειτόνων (neighbors) και το Υ περιέχει τις πραγματικές αξιολογήσεις των χρηστών.

....
for cluster_idx in range(L):
    print(f"Training MLP for cluster {cluster_idx}...")

    # Get users in the current cluster
    cluster_users = np.where(cluster_labels == cluster_idx)[0]
    cluster_data = feature_vectors[cluster_users]

    # Find k nearest neighbors for each user in the cluster
    knn = NearestNeighbors(n_neighbors=k, metric=custom_jaccard_coefficient)
    knn.fit(cluster_data)
    neighbors = knn.kneighbors(cluster_data, return_distance=False)

    # Prepare training data
    X = []
    y = []

    for user_idx, user_neighbors in enumerate(neighbors):
        X.append(cluster_data[user_neighbors])
        y.append(cluster_data[user_idx])

    X = np.array(X).reshape(len(cluster_users), -1)
    y = np.array(y)
....

Στη συνέχεια, χρησιμοποιούμε τη συνάρτηση latexmath:[np.hstack] για να συνενώσουμε οριζόντια τα χαρακτηριστικά (X) και τις ετικέτες (y) σε ένα ενιαίο πίνακα δεδομένων (data).

....
   # Horizontal concatenation of feature vectors (neighbors) and target vectors (user preferences)
    data = np.hstack((X, y))
....

Χρησιμοποιούμε το latexmath:[train_test_split] από τη βιβλιοθήκη sklearn για να χωρίσουμε τα δεδομένα σε training set (90%) και testing set (10%) και εκπαιδεύουμε το μοντέλο.

....
    # Random split into training (90%) and testing (10%) sets
    train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)

    # Split into features and labels
    X_train = train_data[:, :-y.shape[1]]
    y_train = train_data[:, -y.shape[1]:]
    X_test = test_data[:, :-y.shape[1]]
    y_test = test_data[:, -y.shape[1]:]

    # Build and train MLP
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1],)))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(y_train.shape[1], activation='linear'))

    model.compile(optimizer='adam', loss='mean_squared_error')
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)

    mlp_models.append(model)
....

Χρησιμοποιούμε τη μέθοδο latexmath:[predict] για να προβλέψουμε τις τιμές τόσο για το training όσο και για το testing set.

....

    # Evaluate the model using MAE
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

....

Τέλος, υπολογίζουμε το μέσο απόλυτο σφάλμα (Mean Absolute Error, MAE) χρησιμοποιώντας τη συνάρτηση latexmath:[mean_absolute_error] από τη βιβλιοθήκη latexmath:[sklearn.metrics].

....

    train_mae = mean_absolute_error(y_train, y_train_pred)
    test_mae = mean_absolute_error(y_test, y_test_pred)

    # Store the results
    results.append({
        'Cluster': cluster_idx,
        'Train MAE': train_mae,
        'Test MAE': test_mae
    })

# Print the results
results_df = pd.DataFrame(results)
print(results_df)

# Save models
for idx, model in enumerate(mlp_models):
    model.save(f'mlp_cluster_{idx}.keras')

print("Training complete!")
....

==== Ερμηνεία Αποτελεσμάτων

Τα αποτελέσματα που θα πάρουμε έχουν παρόμοια μορφή με τα αποτελέσματα του προηγούμενου ερωτήματος (2), με τη διαφορά της εκτύπωσης του πίνακα latexmath:[results_df] που περιέχει την ακρίβεια εκπαίδευσης (Train MAE) και ελέγχου (Test MAE).

Ο πίνακας αυτός δείχνει το μέσο απόλυτο σφάλμα (MAE) για την εκπαίδευση και τον έλεγχο για κάθε cluster, παρέχοντας έτσι μια καλή εικόνα για την ακρίβεια των νευρωνικών δικτύων που εκπαιδεύτηκαν.

.Train MAE and Test MAE Results
image::3.3.png[]